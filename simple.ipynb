{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08828c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from kornia.utils import create_meshgrid\n",
    "from einops import rearrange\n",
    "import math\n",
    "import argparse\n",
    "from mamba_ssm import Mamba\n",
    "from functools import partial\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.layernorm_gated import RMSNorm, LayerNorm\n",
    "except ImportError:\n",
    "    RMSNorm, LayerNorm = None, None\n",
    "from pytorch_lightning.profiler import SimpleProfiler, PassThroughProfiler\n",
    "from loguru import logger\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "INF = 1e9\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "class GRN(nn.Module):\n",
    "    \"\"\" GRN (Global Response Normalization) layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" ConvNeXtV2 Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(4 * dim)\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXtV2(nn.Module):\n",
    "    \"\"\" ConvNeXt V2\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans=3, num_classes=1000,\n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768],\n",
    "                 drop_path_rate=0., head_init_scale=1.\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.depths = depths\n",
    "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward_features_8(self, x):\n",
    "        feat = {}\n",
    "        x = self.downsample_layers[0](x)\n",
    "        x = self.stages[0](x)\n",
    "        feat[4] = x\n",
    "        x = self.downsample_layers[1](x)\n",
    "        x = self.stages[1](x)\n",
    "        feat[8] = x\n",
    "        return feat  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward_features_4(self, x):\n",
    "        feat = {}\n",
    "        x = self.downsample_layers[0](x)\n",
    "        x = self.stages[0](x)\n",
    "        feat[4] = x\n",
    "        return feat  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "class CovNextV2_nano(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = model = ConvNeXtV2(depths=[2, 2, 8, 2], dims=[80, 160, 320, 640])\n",
    "        self.cnn.norm = None\n",
    "        self.cnn.head = None\n",
    "        self.cnn.downsample_layers[2] = None\n",
    "        self.cnn.downsample_layers[3] = None\n",
    "        self.cnn.stages[2] = None\n",
    "        self.cnn.stages[3] = None\n",
    "\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            'https://github.com/leoluxxx/JamMa/releases/download/v0.1/convnextv2_nano_pretrain.ckpt',\n",
    "            file_name='convnextv2_nano_pretrain.ckpt')\n",
    "        self.cnn.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        self.lin_4 = nn.Conv2d(80, 128, 1)\n",
    "        self.lin_8 = nn.Conv2d(160, 256, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        B, _, H, W = data['imagec_0'].shape\n",
    "        x = torch.cat([data['imagec_0'], data['imagec_1']], 0)\n",
    "        feature_pyramid = self.cnn.forward_features_8(x)\n",
    "        feat_8_0, feat_8_1 = self.lin_8(feature_pyramid[8]).split(B)\n",
    "        feat_4_0, feat_4_1 = self.lin_4(feature_pyramid[4]).split(B)\n",
    "\n",
    "        scale = 8\n",
    "        h_8, w_8 = H//scale, W//scale\n",
    "        device = data['imagec_0'].device\n",
    "        grid = [rearrange((create_meshgrid(h_8, w_8, False, device) * scale).squeeze(0), 'h w t->(h w) t')] * B  # kpt_xy\n",
    "        grid_8 = torch.stack(grid, 0)\n",
    "\n",
    "        data.update({\n",
    "            'bs': B,\n",
    "            'c': feat_8_0.shape[1],\n",
    "            'h_8': h_8,\n",
    "            'w_8': w_8,\n",
    "            'hw_8': h_8 * w_8,\n",
    "            'feat_8_0': feat_8_0,\n",
    "            'feat_8_1': feat_8_1,\n",
    "            'feat_4_0': feat_4_0,\n",
    "            'feat_4_1': feat_4_1,\n",
    "            'grid_8': grid_8,\n",
    "        })\n",
    "\n",
    "class GLU_3(nn.Module):\n",
    "    def __init__(self, dim, mid_dim):\n",
    "        super(GLU_3, self).__init__()\n",
    "        self.W = nn.Conv2d(dim, mid_dim, kernel_size=3, padding=1, bias=False)\n",
    "        self.V = nn.Conv2d(dim, mid_dim, kernel_size=3, padding=1, bias=False)\n",
    "        self.W2 = nn.Conv2d(mid_dim, dim, kernel_size=3, padding=1, bias=False)\n",
    "        self.act = nn.ReLU(True)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, feat):\n",
    "        feat_act = self.act(self.W(feat))\n",
    "        feat_linear = self.V(feat)\n",
    "        feat = feat_act * feat_linear\n",
    "        feat = self.W2(feat)\n",
    "        return feat\n",
    "        # return self.V(feat)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False, drop_path=0.,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
    "\n",
    "        This Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA/MLP -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Add -> LN -> Mixer, returning both\n",
    "        the hidden_states (output of the mixer) and the residual.\n",
    "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(\n",
    "                self.norm, (nn.LayerNorm, RMSNorm)\n",
    "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "\n",
    "    def forward(\n",
    "            self, desc, inference_params=None\n",
    "    ):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        hidden_states = self.norm(desc.to(dtype=self.norm.weight.dtype))\n",
    "        if self.residual_in_fp32:\n",
    "            desc = desc.to(torch.float32)\n",
    "        hidden_states = self.mixer(hidden_states, inference_params=inference_params)\n",
    "        return desc + hidden_states\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "\n",
    "def create_block(\n",
    "    d_model,\n",
    "    ssm_cfg=None,\n",
    "    norm_epsilon=1e-5,\n",
    "    drop_path=0.,\n",
    "    rms_norm=False,\n",
    "    residual_in_fp32=False,\n",
    "    fused_add_norm=False,\n",
    "    layer_idx=None,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    "    if_bimamba=False,\n",
    "    bimamba_type=\"none\",\n",
    "    if_devide_out=False,\n",
    "    init_layer_scale=None,\n",
    "):\n",
    "    if if_bimamba:\n",
    "        bimamba_type = \"v1\"\n",
    "    if ssm_cfg is None:\n",
    "        ssm_cfg = {}\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    mixer_cls = partial(Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)\n",
    "    norm_cls = partial(\n",
    "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
    "    )\n",
    "    block = Block(\n",
    "        d_model,\n",
    "        mixer_cls,\n",
    "        norm_cls=norm_cls,\n",
    "        fused_add_norm=fused_add_norm,\n",
    "        residual_in_fp32=residual_in_fp32,\n",
    "    )\n",
    "    block.layer_idx = layer_idx\n",
    "    return block\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(\n",
    "    module,\n",
    "    n_layer,\n",
    "    initializer_range=0.02,  # Now only used for embedding layer.\n",
    "    rescale_prenorm_residual=True,\n",
    "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
    "):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if module.bias is not None:\n",
    "            if not getattr(module.bias, \"_no_reinit\", False):\n",
    "                nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
    "                # We need to reinit p since this code could be called multiple times\n",
    "                # Having just p *= scale would repeatedly scale it down\n",
    "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "                with torch.no_grad():\n",
    "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
    "\n",
    "def scan_jego(desc0, desc1, step_size):\n",
    "    desc_2w, desc_2h = torch.cat([desc0, desc1], 3), torch.cat([desc0, desc1], 2)\n",
    "    _, _, org_h, org_2w = desc_2w.shape\n",
    "    B, C, org_2h, org_w = desc_2h.shape\n",
    "\n",
    "    H = org_h // step_size\n",
    "    W = org_2w // step_size\n",
    "\n",
    "    xs = desc_2w.new_empty((B, 4, C, H*W))\n",
    "\n",
    "    xs[:, 0] = desc_2w[:, :, ::step_size, ::step_size].contiguous().view(B, C, -1)  # [h/2, 2w/2]\n",
    "    xs[:, 1] = desc_2h.transpose(dim0=2, dim1=3)[:, :, 1::step_size, 1::step_size].contiguous().view(B, C, -1)  # [w/2, 2w/2]\n",
    "    xs[:, 2] = desc_2w[:, :, ::step_size, 1::step_size].contiguous().view(B, C, -1).flip([2])  # [h/2, 2w/2]\n",
    "    xs[:, 3] = desc_2h.transpose(dim0=2, dim1=3)[:, :, ::step_size, 1::step_size].contiguous().view(B, C, -1).flip([2])  # [w/2, 2w/2]\n",
    "\n",
    "    xs = xs.view(B, 4, C, -1).transpose(2, 3)\n",
    "    return xs, org_h, org_w\n",
    "\n",
    "def merge_jego(ys, ori_h: int, ori_w: int, step_size=2):\n",
    "    B, K, C, L = ys.shape\n",
    "    H, W = math.ceil(ori_h / step_size), math.ceil(ori_w / step_size)\n",
    "\n",
    "    new_h = H * step_size\n",
    "    new_w = W * step_size\n",
    "\n",
    "    y_2w = torch.zeros((B, C, new_h, 2*new_w), device=ys.device, dtype=ys.dtype)  # ys.new_empty((B, C, new_h, 2*new_w))\n",
    "    y_2h = torch.zeros((B, C, 2*new_h, new_w), device=ys.device, dtype=ys.dtype)  # ys.new_empty((B, C, 2*new_h, new_w))\n",
    "\n",
    "    y_2w[:, :, ::step_size, ::step_size] = ys[:, 0].reshape(B, C, H, 2*W)\n",
    "    y_2h[:, :, 1::step_size, 1::step_size] = ys[:, 1].reshape(B, C, W, 2*H).transpose(dim0=2, dim1=3)\n",
    "    y_2w[:, :, ::step_size, 1::step_size] = ys[:, 2].flip([2]).reshape(B, C, H, 2*W)\n",
    "    y_2h[:, :, 1::step_size, ::step_size] = ys[:, 3].flip([2]).reshape(B, C, W, 2*H).transpose(dim0=2, dim1=3)\n",
    "\n",
    "    if ori_h != new_h or ori_w != new_w:\n",
    "        y_2w = y_2w[:, :, :ori_h, :ori_w].contiguous()\n",
    "        y_2h = y_2h[:, :, :ori_h, :ori_w].contiguous()\n",
    "    desc0_2w, desc1_2w = torch.chunk(y_2w, 2, dim=3)\n",
    "    desc0_2h, desc1_2h = torch.chunk(y_2h, 2, dim=2)\n",
    "    return desc0_2w+desc0_2h, desc1_2w+desc1_2h\n",
    "\n",
    "\n",
    "    B, K, C, L = ys.shape\n",
    "    H, W = math.ceil(ori_h / step_size), math.ceil(ori_w / step_size)\n",
    "\n",
    "    new_h = H * step_size\n",
    "    new_w = W * step_size\n",
    "\n",
    "    y_2w = torch.zeros((B, C, new_h, 2*new_w), device=ys.device, dtype=ys.dtype)  # ys.new_empty((B, C, new_h, 2*new_w))\n",
    "    y_2h = torch.zeros((B, C, 2*new_h, new_w), device=ys.device, dtype=ys.dtype)  # ys.new_empty((B, C, 2*new_h, new_w))\n",
    "\n",
    "    y_2h[:, :, ::step_size, ::step_size] = ys[:, 0].reshape(B, C, 2*H, W)\n",
    "    y_2w[:, :, 1::step_size, 1::step_size] = ys[:, 1].reshape(B, C, 2*W, H).transpose(dim0=2, dim1=3)\n",
    "    y_2h[:, :, ::step_size, 1::step_size] = ys[:, 2].flip([2]).reshape(B, C, 2*H, W)\n",
    "    y_2w[:, :, 1::step_size, ::step_size] = ys[:, 3].flip([2]).reshape(B, C, 2*W, H).transpose(dim0=2, dim1=3)\n",
    "\n",
    "    if ori_h != new_h or ori_w != new_w:\n",
    "        y_2w = y_2w[:, :, :ori_h, :ori_w].contiguous()\n",
    "        y_2h = y_2h[:, :, :ori_h, :ori_w].contiguous()\n",
    "    desc0_2w, desc1_2w = torch.chunk(y_2w, 2, dim=3)\n",
    "    desc0_2h, desc1_2h = torch.chunk(y_2h, 2, dim=2)\n",
    "    return desc0_2w+desc0_2h, desc1_2w+desc1_2h\n",
    "\n",
    "class JointMamba(nn.Module):\n",
    "    def __init__(self, feature_dim: int, depth,\n",
    "                 ssm_cfg=None,\n",
    "                 norm_epsilon: float = 1e-5,\n",
    "                 rms_norm: bool = False,\n",
    "                 initializer_cfg=None,\n",
    "                 fused_add_norm=False,\n",
    "                 residual_in_fp32=False,\n",
    "                 if_bimamba=False,\n",
    "                 bimamba_type=\"none\",\n",
    "                 if_devide_out=False,\n",
    "                 init_layer_scale=None,\n",
    "                 profiler=None):\n",
    "        super().__init__()\n",
    "        self.profiler = profiler or PassThroughProfiler()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.num_layers = depth\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.layers.append(create_block(\n",
    "                    feature_dim,\n",
    "                    ssm_cfg=ssm_cfg,\n",
    "                    norm_epsilon=norm_epsilon,\n",
    "                    rms_norm=rms_norm,\n",
    "                    residual_in_fp32=residual_in_fp32,\n",
    "                    fused_add_norm=fused_add_norm,\n",
    "                    layer_idx=i,\n",
    "                    if_bimamba=if_bimamba,\n",
    "                    bimamba_type=bimamba_type,\n",
    "                    if_devide_out=if_devide_out,\n",
    "                    init_layer_scale=init_layer_scale,\n",
    "                ))\n",
    "        # mamba init\n",
    "        self.apply(\n",
    "            partial(\n",
    "                _init_weights,\n",
    "                n_layer=depth,\n",
    "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
    "            )\n",
    "        )\n",
    "        self.aggregator = GLU_3(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        desc0, desc1 = data['feat_8_0'], data['feat_8_1']\n",
    "        desc0, desc1 = desc0.view(data['bs'], -1, data['h_8'], data['w_8']), desc1.view(data['bs'], -1, data['h_8'], data['w_8'])\n",
    "        x, ori_h, ori_w = scan_jego(desc0, desc1, 2)\n",
    "        for i in range(len(self.layers) // 4):\n",
    "            y0 = self.layers[i * 4](x[:, 0])\n",
    "            y1 = self.layers[i * 4 + 1](x[:, 1])\n",
    "            y2 = self.layers[i * 4 + 2](x[:, 2])\n",
    "            y3 = self.layers[i * 4 + 3](x[:, 3])\n",
    "            y = torch.stack([y0, y1, y2, y3], 1).transpose(2, 3)\n",
    "        desc0, desc1 = merge_jego(y, ori_h, ori_w, 2)\n",
    "        desc = self.aggregator(torch.cat([desc0, desc1], 0))\n",
    "        desc0, desc1 = torch.chunk(desc, 2, dim=0)\n",
    "        desc0, desc1 = desc0.flatten(2, 3), desc1.flatten(2, 3)\n",
    "        data.update({\n",
    "            'feat_8_0': desc0,\n",
    "            'feat_8_1': desc1,\n",
    "        })\n",
    "\n",
    "def mask_border(m, b: int, v):\n",
    "    \"\"\" Mask borders with value\n",
    "    Args:\n",
    "        m (torch.Tensor): [N, H0, W0, H1, W1]\n",
    "        b (int)\n",
    "        v (m.dtype)\n",
    "    \"\"\"\n",
    "    if b <= 0:\n",
    "        return\n",
    "\n",
    "    m[:, :b] = v\n",
    "    m[:, :, :b] = v\n",
    "    m[:, :, :, :b] = v\n",
    "    m[:, :, :, :, :b] = v\n",
    "    m[:, -b:] = v\n",
    "    m[:, :, -b:] = v\n",
    "    m[:, :, :, -b:] = v\n",
    "    m[:, :, :, :, -b:] = v\n",
    "\n",
    "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n",
    "    if bd <= 0:\n",
    "        return\n",
    "\n",
    "    m[:, :bd] = v\n",
    "    m[:, :, :bd] = v\n",
    "    m[:, :, :, :bd] = v\n",
    "    m[:, :, :, :, :bd] = v\n",
    "\n",
    "    h0s, w0s = p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int()\n",
    "    h1s, w1s = p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int()\n",
    "    for b_idx, (h0, w0, h1, w1) in enumerate(zip(h0s, w0s, h1s, w1s)):\n",
    "        m[b_idx, h0 - bd:] = v\n",
    "        m[b_idx, :, w0 - bd:] = v\n",
    "        m[b_idx, :, :, h1 - bd:] = v\n",
    "        m[b_idx, :, :, :, w1 - bd:] = v\n",
    "\n",
    "def compute_max_candidates(p_m0, p_m1):\n",
    "    \"\"\"Compute the max candidates of all pairs within a batch\n",
    "\n",
    "    Args:\n",
    "        p_m0, p_m1 (torch.Tensor): padded masks\n",
    "    \"\"\"\n",
    "    h0s, w0s = p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0]\n",
    "    h1s, w1s = p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0]\n",
    "    max_cand = torch.sum(\n",
    "        torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n",
    "    return max_cand\n",
    "\n",
    "def generate_random_mask(n, num_true):\n",
    "    # 创建全 False 的掩码\n",
    "    mask = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "    # 随机选择 num_true 个位置并设置为 True\n",
    "    indices = torch.randperm(n)[:num_true]\n",
    "    mask[indices] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "class CoarseMatching(nn.Module):\n",
    "    def __init__(self, config, profiler):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # general config\n",
    "        d_model = 256\n",
    "        self.thr = config['thr']\n",
    "        self.use_sm = config['use_sm']\n",
    "        self.inference = config['inference']\n",
    "        self.border_rm = config['border_rm']\n",
    "\n",
    "        self.final_proj = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        self.temperature = config['dsmax_temperature']\n",
    "        self.profiler = profiler\n",
    "\n",
    "    def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n",
    "        feat_c0 = self.final_proj(feat_c0)\n",
    "        feat_c1 = self.final_proj(feat_c1)\n",
    "\n",
    "        # normalize\n",
    "        feat_c0, feat_c1 = map(lambda feat: feat / feat.shape[-1] ** .5,\n",
    "                               [feat_c0, feat_c1])\n",
    "\n",
    "        sim_matrix = torch.einsum(\"nlc,nsc->nls\", feat_c0,\n",
    "                                  feat_c1) / self.temperature\n",
    "        if mask_c0 is not None:\n",
    "            sim_matrix.masked_fill_(\n",
    "                ~(mask_c0[..., None] * mask_c1[:, None]).bool(),\n",
    "                -INF)\n",
    "        if self.inference:\n",
    "            # predict coarse matches from conf_matrix\n",
    "            data.update(**self.get_coarse_match_inference(sim_matrix, data))\n",
    "        else:\n",
    "            conf_matrix_0_to_1 = F.softmax(sim_matrix, 2)\n",
    "            conf_matrix_1_to_0 = F.softmax(sim_matrix, 1)\n",
    "            data.update({'conf_matrix_0_to_1': conf_matrix_0_to_1,\n",
    "                         'conf_matrix_1_to_0': conf_matrix_1_to_0\n",
    "                         })\n",
    "            # predict coarse matches from conf_matrix\n",
    "            data.update(**self.get_coarse_match_training(conf_matrix_0_to_1, conf_matrix_1_to_0, data))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_coarse_match_training(self, conf_matrix_0_to_1, conf_matrix_1_to_0, data):\n",
    "        axes_lengths = {\n",
    "            'h0c': data['hw0_c'][0],\n",
    "            'w0c': data['hw0_c'][1],\n",
    "            'h1c': data['hw1_c'][0],\n",
    "            'w1c': data['hw1_c'][1]\n",
    "        }\n",
    "        _device = conf_matrix_0_to_1.device\n",
    "\n",
    "        # confidence thresholding\n",
    "        # {(nearest neighbour for 0 to 1) U (nearest neighbour for 1 to 0)}\n",
    "        mask = torch.logical_or(\n",
    "            (conf_matrix_0_to_1 > self.thr) * (conf_matrix_0_to_1 == conf_matrix_0_to_1.max(dim=2, keepdim=True)[0]),\n",
    "            (conf_matrix_1_to_0 > self.thr) * (conf_matrix_1_to_0 == conf_matrix_1_to_0.max(dim=1, keepdim=True)[0]))\n",
    "\n",
    "        mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c',\n",
    "                         **axes_lengths)\n",
    "        if 'mask0' not in data:\n",
    "            mask_border(mask, self.border_rm, False)\n",
    "        else:\n",
    "            mask_border_with_padding(mask, self.border_rm, False,\n",
    "                                     data['mask0'], data['mask1'])\n",
    "        mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)',\n",
    "                         **axes_lengths)\n",
    "\n",
    "        # find all valid coarse matches\n",
    "        b_ids, i_ids, j_ids = mask.nonzero(as_tuple=True)\n",
    "\n",
    "        mconf = torch.maximum(conf_matrix_0_to_1[b_ids, i_ids, j_ids], conf_matrix_1_to_0[b_ids, i_ids, j_ids])\n",
    "\n",
    "        # random sampling of training samples for fine-level XoFTR\n",
    "        # (optional) pad samples with gt coarse-level matches\n",
    "        if self.training:\n",
    "            # NOTE:\n",
    "            # the sampling is performed across all pairs in a batch without manually balancing\n",
    "            # samples for fine-level increases w.r.t. batch_size\n",
    "            if 'mask0' not in data:\n",
    "                num_candidates_max = mask.size(0) * max(\n",
    "                    mask.size(1), mask.size(2))\n",
    "            else:\n",
    "                num_candidates_max = compute_max_candidates(\n",
    "                    data['mask0'], data['mask1'])\n",
    "            num_matches_train = int(num_candidates_max *\n",
    "                                    self.config['train_coarse_percent'])\n",
    "            num_matches_pred = len(b_ids)\n",
    "            train_pad_num_gt_min = self.config['train_pad_num_gt_min']\n",
    "            assert train_pad_num_gt_min < num_matches_train, \"min-num-gt-pad should be less than num-train-matches\"\n",
    "\n",
    "            # pred_indices is to select from prediction\n",
    "            if num_matches_pred <= num_matches_train - train_pad_num_gt_min:\n",
    "                pred_indices = torch.arange(num_matches_pred, device=_device)\n",
    "            else:\n",
    "                pred_indices = torch.randint(\n",
    "                    num_matches_pred,\n",
    "                    (num_matches_train - train_pad_num_gt_min,),\n",
    "                    device=_device)\n",
    "\n",
    "            # gt_pad_indices is to select from gt padding. e.g. max(3787-4800, 200)\n",
    "            gt_pad_indices = torch.randint(\n",
    "                len(data['spv_b_ids']),\n",
    "                (max(num_matches_train - num_matches_pred,\n",
    "                     train_pad_num_gt_min),),\n",
    "                device=_device)\n",
    "            mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)  # set conf of gt paddings to all zero\n",
    "\n",
    "            b_ids, i_ids, j_ids, mconf = map(\n",
    "                lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]],\n",
    "                                       dim=0),\n",
    "                *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']],\n",
    "                     [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n",
    "\n",
    "        # these matches are selected patches that feed into fine-level network\n",
    "        coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n",
    "\n",
    "        # update with matches in original image resolution\n",
    "        scale = data['hw0_i'][0] / data['hw0_c'][0]\n",
    "        scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n",
    "        scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n",
    "        mkpts0_c = torch.stack(\n",
    "            [i_ids % data['hw0_c'][1], torch.div(i_ids, data['hw0_c'][1], rounding_mode='trunc')],\n",
    "            dim=1) * scale0\n",
    "        mkpts1_c = torch.stack(\n",
    "            [j_ids % data['hw1_c'][1], torch.div(j_ids, data['hw1_c'][1], rounding_mode='trunc')],\n",
    "            dim=1) * scale1\n",
    "\n",
    "        # these matches is the current prediction (for visualization)\n",
    "        coarse_matches.update({\n",
    "            'gt_mask': mconf == 0,\n",
    "            'm_bids': b_ids[mconf != 0],  # mconf == 0 => gt matches\n",
    "            'mkpts0_c': mkpts0_c[mconf != 0],\n",
    "            'mkpts1_c': mkpts1_c[mconf != 0],\n",
    "            'mkpts0_c_train': mkpts0_c,\n",
    "            'mkpts1_c_train': mkpts1_c,\n",
    "            'mconf': mconf[mconf != 0]\n",
    "        })\n",
    "\n",
    "        return coarse_matches\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_coarse_match_inference(self, sim_matrix, data):\n",
    "        axes_lengths = {\n",
    "            'h0c': data['hw0_c'][0],\n",
    "            'w0c': data['hw0_c'][1],\n",
    "            'h1c': data['hw1_c'][0],\n",
    "            'w1c': data['hw1_c'][1]\n",
    "        }\n",
    "        # softmax for 0 to 1\n",
    "        conf_matrix_ = F.softmax(sim_matrix, 2) if self.use_sm else sim_matrix\n",
    "\n",
    "        # confidence thresholding and nearest neighbour for 0 to 1\n",
    "        mask = (conf_matrix_ > self.thr) * (conf_matrix_ == conf_matrix_.max(dim=2, keepdim=True)[0])\n",
    "\n",
    "        # unlike training, reuse the same conf martix to decrease the vram consumption\n",
    "        # softmax for 0 to 1\n",
    "        conf_matrix_ = F.softmax(sim_matrix, 1) if self.use_sm else sim_matrix\n",
    "\n",
    "        # update mask {(nearest neighbour for 0 to 1) U (nearest neighbour for 1 to 0)}\n",
    "        mask = torch.logical_or(mask,\n",
    "                                (conf_matrix_ > self.thr) * (conf_matrix_ == conf_matrix_.max(dim=1, keepdim=True)[0]))\n",
    "\n",
    "        mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c',\n",
    "                         **axes_lengths)\n",
    "        if 'mask0' not in data:\n",
    "            mask_border(mask, self.border_rm, False)\n",
    "        else:\n",
    "            mask_border_with_padding(mask, self.border_rm, False,\n",
    "                                     data['mask0'], data['mask1'])\n",
    "        mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)',\n",
    "                         **axes_lengths)\n",
    "\n",
    "        # find all valid coarse matches\n",
    "        b_ids, i_ids, j_ids = mask.nonzero(as_tuple=True)\n",
    "\n",
    "        mconf = sim_matrix[b_ids, i_ids, j_ids]\n",
    "\n",
    "        # these matches are selected patches that feed into fine-level network\n",
    "        coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n",
    "\n",
    "        # update with matches in original image resolution\n",
    "        scale = data['hw0_i'][0] / data['hw0_c'][0]\n",
    "        scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n",
    "        scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n",
    "        mkpts0_c = torch.stack(\n",
    "            [i_ids % data['hw0_c'][1], torch.div(i_ids, data['hw0_c'][1], rounding_mode='trunc')],\n",
    "            dim=1) * scale0\n",
    "        mkpts1_c = torch.stack(\n",
    "            [j_ids % data['hw1_c'][1], torch.div(j_ids, data['hw1_c'][1], rounding_mode='trunc')],\n",
    "            dim=1) * scale1\n",
    "\n",
    "        # these matches are the current coarse level predictions\n",
    "        coarse_matches.update({\n",
    "            'mconf': mconf,\n",
    "            'm_bids': b_ids,  # mconf == 0 => gt matches\n",
    "            'mkpts0_c': mkpts0_c,\n",
    "            'mkpts1_c': mkpts1_c,\n",
    "        })\n",
    "\n",
    "        return coarse_matches\n",
    "\n",
    "class FineSubMatching(nn.Module):\n",
    "    \"\"\"Fine-level and Sub-pixel matching\"\"\"\n",
    "\n",
    "    def __init__(self, config, profiler):\n",
    "        super().__init__()\n",
    "        self.temperature = config['fine']['dsmax_temperature']\n",
    "        self.W_f = config['fine_window_size']\n",
    "        self.inference = config['fine']['inference']\n",
    "        dim_f = 64\n",
    "        self.fine_thr = config['fine']['thr']\n",
    "        self.fine_proj = nn.Linear(dim_f, dim_f, bias=False)\n",
    "        self.subpixel_mlp = nn.Sequential(nn.Linear(2 * dim_f, 2 * dim_f, bias=False),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(2 * dim_f, 4, bias=False))\n",
    "        self.fine_spv_max = 500  # saving memory\n",
    "        self.profiler = profiler\n",
    "\n",
    "    def forward(self, feat_f0_unfold, feat_f1_unfold, data):\n",
    "        M, WW, C = feat_f0_unfold.shape\n",
    "        W_f = self.W_f\n",
    "\n",
    "        # corner case: if no coarse matches found\n",
    "        if M == 0:\n",
    "            assert self.training == False, \"M is always >0, when training, see coarse_matching.py\"\n",
    "            logger.warning('No matches found in coarse-level.')\n",
    "            if self.inference:\n",
    "                data.update({\n",
    "                    'mkpts0_f': data['mkpts0_c'],\n",
    "                    'mkpts1_f': data['mkpts1_c'],\n",
    "                    'mconf_f': torch.zeros(0, device=feat_f0_unfold.device),\n",
    "                })\n",
    "            else:\n",
    "                data.update({\n",
    "                    'mkpts0_f': data['mkpts0_c'],\n",
    "                    'mkpts1_f': data['mkpts1_c'],\n",
    "                    'mconf_f': torch.zeros(0, device=feat_f0_unfold.device),\n",
    "                    'mkpts0_f_train': data['mkpts0_c_train'],\n",
    "                    'mkpts1_f_train': data['mkpts1_c_train'],\n",
    "                    'conf_matrix_fine': torch.zeros(1, W_f * W_f, W_f * W_f, device=feat_f0_unfold.device),\n",
    "                    'b_ids_fine': torch.zeros(0, device=feat_f0_unfold.device),\n",
    "                    'i_ids_fine': torch.zeros(0, device=feat_f0_unfold.device),\n",
    "                    'j_ids_fine': torch.zeros(0, device=feat_f0_unfold.device),\n",
    "                })\n",
    "            return\n",
    "\n",
    "        feat_f0 = self.fine_proj(feat_f0_unfold)\n",
    "        feat_f1 = self.fine_proj(feat_f1_unfold)\n",
    "\n",
    "        # normalize\n",
    "        feat_f0, feat_f1 = map(lambda feat: feat / feat.shape[-1] ** .5,\n",
    "                               [feat_f0, feat_f1])\n",
    "        sim_matrix = torch.einsum(\"nlc,nsc->nls\", feat_f0,\n",
    "                                  feat_f1) / self.temperature\n",
    "\n",
    "        conf_matrix_fine = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n",
    "\n",
    "        # predict fine-level and sub-pixel matches from conf_matrix\n",
    "        data.update(**self.get_fine_sub_match(conf_matrix_fine, feat_f0_unfold, feat_f1_unfold, data))\n",
    "\n",
    "    def get_fine_sub_match(self, conf_matrix_fine, feat_f0_unfold, feat_f1_unfold, data):\n",
    "        with torch.no_grad():\n",
    "            W_f = self.W_f\n",
    "\n",
    "            # 1. confidence thresholding\n",
    "            mask = conf_matrix_fine > self.fine_thr\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                mask[0, 0, 0] = 1\n",
    "                conf_matrix_fine[0, 0, 0] = 1\n",
    "\n",
    "            # match only the highest confidence\n",
    "            mask = mask \\\n",
    "                   * (conf_matrix_fine == conf_matrix_fine.amax(dim=[1, 2], keepdim=True))\n",
    "\n",
    "            # 3. find all valid fine matches\n",
    "            # this only works when at most one `True` in each row\n",
    "            mask_v, all_j_ids = mask.max(dim=2)\n",
    "            b_ids, i_ids = torch.where(mask_v)\n",
    "            j_ids = all_j_ids[b_ids, i_ids]\n",
    "            mconf = conf_matrix_fine[b_ids, i_ids, j_ids]\n",
    "\n",
    "            # 4. update with matches in original image resolution\n",
    "\n",
    "            # indices from coarse matches\n",
    "            b_ids_c, i_ids_c, j_ids_c = data['b_ids'], data['i_ids'], data['j_ids']\n",
    "\n",
    "            # scale (coarse level / fine-level)\n",
    "            scale_f_c = data['hw0_f'][0] // data['hw0_c'][0]\n",
    "\n",
    "            # coarse level matches scaled to fine-level (1/2)\n",
    "            mkpts0_c_scaled_to_f = torch.stack(\n",
    "                [i_ids_c % data['hw0_c'][1], torch.div(i_ids_c, data['hw0_c'][1], rounding_mode='trunc')],\n",
    "                dim=1) * scale_f_c\n",
    "\n",
    "            mkpts1_c_scaled_to_f = torch.stack(\n",
    "                [j_ids_c % data['hw1_c'][1], torch.div(j_ids_c, data['hw1_c'][1], rounding_mode='trunc')],\n",
    "                dim=1) * scale_f_c\n",
    "\n",
    "            # updated b_ids after second thresholding\n",
    "            updated_b_ids = b_ids_c[b_ids]\n",
    "\n",
    "            # scales (image res / fine level)\n",
    "            scale = data['hw0_i'][0] / data['hw0_f'][0]\n",
    "            scale0 = scale * data['scale0'][updated_b_ids] if 'scale0' in data else scale\n",
    "            scale1 = scale * data['scale1'][updated_b_ids] if 'scale1' in data else scale\n",
    "\n",
    "            # fine-level discrete matches on window coordiantes\n",
    "            mkpts0_f_window = torch.stack(\n",
    "                [i_ids % W_f, torch.div(i_ids, W_f, rounding_mode='trunc')],\n",
    "                dim=1)\n",
    "\n",
    "            mkpts1_f_window = torch.stack(\n",
    "                [j_ids % W_f, torch.div(j_ids, W_f, rounding_mode='trunc')],\n",
    "                dim=1)\n",
    "\n",
    "        # sub-pixel refinement\n",
    "        sub_ref = self.subpixel_mlp(torch.cat([feat_f0_unfold[b_ids, i_ids], feat_f1_unfold[b_ids, j_ids]], dim=-1))\n",
    "        sub_ref0, sub_ref1 = torch.chunk(sub_ref, 2, dim=1)\n",
    "        sub_ref0, sub_ref1 = sub_ref0.squeeze(1), sub_ref1.squeeze(1)\n",
    "        sub_ref0 = torch.tanh(sub_ref0) * 0.5\n",
    "        sub_ref1 = torch.tanh(sub_ref1) * 0.5\n",
    "\n",
    "        pad = 0 if W_f % 2 == 0 else W_f // 2\n",
    "        # final sub-pixel matches by (coarse-level + fine-level windowed + sub-pixel refinement)\n",
    "        mkpts0_f1 = (mkpts0_f_window + mkpts0_c_scaled_to_f[b_ids] - pad) * scale0  # + sub_ref0\n",
    "        mkpts1_f1 = (mkpts1_f_window + mkpts1_c_scaled_to_f[b_ids] - pad) * scale1  # + sub_ref1\n",
    "        mkpts0_f_train = mkpts0_f1 + sub_ref0 * scale0  # + sub_ref0\n",
    "        mkpts1_f_train = mkpts1_f1 + sub_ref1 * scale1  # + sub_ref1\n",
    "        mkpts0_f = mkpts0_f_train.clone().detach()\n",
    "        mkpts1_f = mkpts1_f_train.clone().detach()\n",
    "\n",
    "        # These matches is the current prediction (for visualization)\n",
    "        sub_pixel_matches = {\n",
    "            'm_bids': b_ids_c[b_ids[mconf != 0]],  # mconf == 0 => gt matches\n",
    "            'mkpts0_f1': mkpts0_f1[mconf != 0],\n",
    "            'mkpts1_f1': mkpts1_f1[mconf != 0],\n",
    "            'mkpts0_f': mkpts0_f[mconf != 0],\n",
    "            'mkpts1_f': mkpts1_f[mconf != 0],\n",
    "            'mconf_f': mconf[mconf != 0]\n",
    "        }\n",
    "\n",
    "        # These matches are used for training\n",
    "        if not self.inference:\n",
    "            if self.fine_spv_max is None or self.fine_spv_max > len(data['b_ids']):\n",
    "                sub_pixel_matches.update({\n",
    "                    'mkpts0_f_train': mkpts0_f_train,\n",
    "                    'mkpts1_f_train': mkpts1_f_train,\n",
    "                    'b_ids_fine': data['b_ids'],\n",
    "                    'i_ids_fine': data['i_ids'],\n",
    "                    'j_ids_fine': data['j_ids'],\n",
    "                    'conf_matrix_fine': conf_matrix_fine\n",
    "                })\n",
    "            else:\n",
    "                train_mask = generate_random_mask(len(data['b_ids']), self.fine_spv_max)\n",
    "                sub_pixel_matches.update({\n",
    "                    'mkpts0_f_train': mkpts0_f_train,\n",
    "                    'mkpts1_f_train': mkpts1_f_train,\n",
    "                    'b_ids_fine': data['b_ids'][train_mask],\n",
    "                    'i_ids_fine': data['i_ids'][train_mask],\n",
    "                    'j_ids_fine': data['j_ids'][train_mask],\n",
    "                    'conf_matrix_fine': conf_matrix_fine[train_mask]\n",
    "                })\n",
    "\n",
    "        return sub_pixel_matches\n",
    "\n",
    "class KeypointEncoder_wo_score(nn.Module):\n",
    "    \"\"\" Joint encoding of visual appearance and location using MLPs\"\"\"\n",
    "    def __init__(self, feature_dim, layers):\n",
    "        super().__init__()\n",
    "        self.encoder = MLP([2] + layers + [feature_dim])\n",
    "        nn.init.constant_(self.encoder[-1].bias, 0.0)\n",
    "\n",
    "    def forward(self, kpts):\n",
    "        return self.encoder(kpts)\n",
    "\n",
    "@torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
    "def normalize_keypoints(\n",
    "        kpts: torch.Tensor,\n",
    "        size: torch.Tensor) -> torch.Tensor:\n",
    "    if not isinstance(size, torch.Tensor):\n",
    "        size = torch.tensor(size, device=kpts.device, dtype=kpts.dtype)\n",
    "    size = size.to(kpts)\n",
    "    shift = size / 2\n",
    "    scale = size.max(-1).values / 2\n",
    "    kpts = (kpts - shift[..., None, :]) / scale[..., None, None]\n",
    "    return kpts\n",
    "\n",
    "class up_conv4(nn.Module):\n",
    "    def __init__(self, dim_in, dim_mid, dim_out):\n",
    "        super(up_conv4, self).__init__()\n",
    "        self.lin = nn.Conv2d(dim_in, dim_mid, kernel_size=3, stride=1, padding=1)\n",
    "        self.inter = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.transconv = nn.ConvTranspose2d(dim_in, dim_mid, kernel_size=2, stride=2)\n",
    "        self.cbr = nn.Sequential(\n",
    "            nn.Conv2d(dim_mid, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_inter = self.inter(self.lin(x))\n",
    "        x_conv = self.transconv(x)\n",
    "        x = self.cbr(x_inter+x_conv)\n",
    "        return x\n",
    "\n",
    "class MLPMixerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, factor=1):\n",
    "        super(MLPMixerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(nn.Linear(dim1, dim1*factor),\n",
    "                                  nn.GELU(),\n",
    "                                  nn.Linear(dim1*factor, dim1))\n",
    "        self.mlp2 = nn.Sequential(nn.Linear(dim2, dim2*factor),\n",
    "                                  nn.LayerNorm(dim2*factor),\n",
    "                                  nn.GELU(),\n",
    "                                  nn.Linear(dim2*factor, dim2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [N, L, C]\n",
    "            x_mask (torch.Tensor): [N, L] (optional)\n",
    "        \"\"\"\n",
    "        x = x + self.mlp1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x + self.mlp2(x)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "class JamMa_Concrete(nn.Module):\n",
    "    def __init__(self, config, profiler=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.profiler = profiler or PassThroughProfiler()\n",
    "        self.d_model_c = self.config['coarse']['d_model']\n",
    "        self.d_model_f = self.config['fine']['d_model']\n",
    "\n",
    "        self.kenc = KeypointEncoder_wo_score(self.d_model_c, [32, 64, 128, self.d_model_c])\n",
    "        self.joint_mamba = JointMamba(self.d_model_c, 4, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, profiler=self.profiler)\n",
    "        self.coarse_matching = CoarseMatching(config['match_coarse'], self.profiler)\n",
    "\n",
    "        self.act = nn.GELU()\n",
    "        dim = [256, 128, 64]\n",
    "        self.up2 = up_conv4(dim[0], dim[1], dim[1])  # 1/8 -> 1/4\n",
    "        self.conv7a = nn.Conv2d(2*dim[1], dim[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7b = nn.Conv2d(dim[1], dim[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.up3 = up_conv4(dim[1], dim[2], dim[2])  # 1/4 -> 1/2\n",
    "        self.conv8a = nn.Conv2d(dim[2], dim[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8b = nn.Conv2d(dim[2], dim[2], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        W = self.config['fine_window_size']\n",
    "        self.fine_enc = nn.ModuleList([MLPMixerEncoderLayer(2*W**2, 64) for _ in range(4)])\n",
    "        self.fine_matching = FineSubMatching(config, self.profiler)\n",
    "\n",
    "    def coarse_match(self, data):\n",
    "        desc0, desc1 = data['feat_8_0'].flatten(2, 3), data['feat_8_1'].flatten(2, 3)\n",
    "        kpts0, kpts1 = data['grid_8'], data['grid_8']\n",
    "        # Keypoint normalization.\n",
    "        kpts0 = normalize_keypoints(kpts0, data['imagec_0'].shape[-2:])\n",
    "        kpts1 = normalize_keypoints(kpts1, data['imagec_1'].shape[-2:])\n",
    "\n",
    "        kpts0, kpts1 = kpts0.transpose(1, 2), kpts1.transpose(1, 2)\n",
    "        desc0 = desc0 + self.kenc(kpts0)\n",
    "        desc1 = desc1 + self.kenc(kpts1)\n",
    "        data.update({\n",
    "            'feat_8_0': desc0,\n",
    "            'feat_8_1': desc1,\n",
    "        })\n",
    "\n",
    "        with self.profiler.profile(\"coarse interaction\"):\n",
    "            self.joint_mamba(data)\n",
    "\n",
    "        # 3. match coarse-level\n",
    "        mask_c0 = mask_c1 = None  # mask is useful in training\n",
    "        if 'mask0' in data:\n",
    "            mask_c0, mask_c1 = data['mask0'].flatten(-2), data['mask1'].flatten(-2)\n",
    "\n",
    "        with self.profiler.profile(\"coarse matching\"):\n",
    "            self.coarse_matching(data['feat_8_0'].transpose(1,2), data['feat_8_1'].transpose(1,2), data, mask_c0=mask_c0, mask_c1=mask_c1)\n",
    "\n",
    "    def inter_fpn(self, feat_8, feat_4):\n",
    "        d2 = self.up2(feat_8)  # 1/4\n",
    "        d2 = self.act(self.conv7a(torch.cat([feat_4, d2], 1)))\n",
    "        feat_4 = self.act(self.conv7b(d2))\n",
    "\n",
    "        d1 = self.up3(feat_4)  # 1/2\n",
    "        d1 = self.act(self.conv8a(d1))\n",
    "        feat_2 = self.conv8b(d1)\n",
    "        return feat_2\n",
    "\n",
    "    def fine_preprocess(self, data, profiler):\n",
    "        data['resolution1'] = 8\n",
    "        stride = data['resolution1'] // self.config['resolution'][1]\n",
    "        W = self.config['fine_window_size']\n",
    "        feat_8 = torch.cat([data['feat_8_0'], data['feat_8_1']], 0).view(2*data['bs'], data['c'], data['h_8'], -1)\n",
    "        feat_4 = torch.cat([data['feat_4_0'], data['feat_4_1']], 0)\n",
    "\n",
    "        if data['b_ids'].shape[0] == 0:\n",
    "            feat0 = torch.empty(0, W ** 2, self.d_model_f, device=feat_4.device)\n",
    "            feat1 = torch.empty(0, W ** 2, self.d_model_f, device=feat_4.device)\n",
    "            return feat0, feat1\n",
    "\n",
    "        # feat_f = self.inter_fpn(feat_8, feat_4, feat_2)\n",
    "        feat_f = self.inter_fpn(feat_8, feat_4)\n",
    "        feat_f0, feat_f1 = torch.chunk(feat_f, 2, dim=0)\n",
    "        data.update({'hw0_f': feat_f0.shape[2:], 'hw1_f': feat_f1.shape[2:]})\n",
    "\n",
    "        # 1. unfold(crop) all local windows\n",
    "        pad = 0 if W % 2 == 0 else W//2\n",
    "        feat_f0_unfold = F.unfold(feat_f0, kernel_size=(W, W), stride=stride, padding=pad)\n",
    "        feat_f0_unfold = rearrange(feat_f0_unfold, 'n (c ww) l -> n l ww c', ww=W ** 2)\n",
    "        feat_f1_unfold = F.unfold(feat_f1, kernel_size=(W, W), stride=stride, padding=pad)\n",
    "        feat_f1_unfold = rearrange(feat_f1_unfold, 'n (c ww) l -> n l ww c', ww=W ** 2)  # [b, h_f/stride * w_f/stride, w*w, c]\n",
    "\n",
    "        # 2. select only the predicted matches\n",
    "        feat_f0_unfold = feat_f0_unfold[data['b_ids'], data['i_ids']]  # [n, ww, cf]\n",
    "        feat_f1_unfold = feat_f1_unfold[data['b_ids'], data['j_ids']]  # [n, ww, cf]\n",
    "\n",
    "        feat_f = torch.cat([feat_f0_unfold, feat_f1_unfold], 1).transpose(1, 2)\n",
    "        for layer in self.fine_enc:\n",
    "            feat_f = layer(feat_f)\n",
    "        feat_f0_unfold, feat_f1_unfold = feat_f[:, :, :W**2], feat_f[:, :, W**2:]\n",
    "        return feat_f0_unfold, feat_f1_unfold\n",
    "\n",
    "    def forward(self, data, mode='test'):\n",
    "        self.mode = mode\n",
    "        data.update({\n",
    "            'hw0_i': data['imagec_0'].shape[2:],\n",
    "            'hw1_i': data['imagec_1'].shape[2:],\n",
    "            'hw0_c': [data['h_8'], data['w_8']],\n",
    "            'hw1_c': [data['h_8'], data['w_8']],\n",
    "        })\n",
    "\n",
    "        self.coarse_match(data)\n",
    "\n",
    "        with self.profiler.profile(\"fine matching\"):\n",
    "            # 4. fine-level matching module\n",
    "            feat_f0_unfold, feat_f1_unfold = self.fine_preprocess(data, self.profiler)\n",
    "\n",
    "            # 5. match fine-level and sub-pixel refinement\n",
    "            self.fine_matching(feat_f0_unfold.transpose(1, 2), feat_f1_unfold.transpose(1, 2), data)\n",
    "\n",
    "def read_megadepth_color(path, resize=None, df=None, padding=False):\n",
    "    # read image\n",
    "    image = Image.open(path)\n",
    "    w, h = image.width, image.height\n",
    "    w_new, h_new = get_resized_wh(image.width, image.height, resize)\n",
    "    w_new, h_new = get_divisible_wh(w_new, h_new, df)\n",
    "    scale = torch.tensor([w/w_new, h/h_new], dtype=torch.float)\n",
    "    resize_fun = transforms.Resize((h_new, w_new), InterpolationMode.BICUBIC)\n",
    "    image = resize_fun(image)\n",
    "    image = np.array(image, dtype=np.float32)\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.repeat(image[..., np.newaxis], 3, axis=2)\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image /= 255.0\n",
    "    image = torch.from_numpy(image)\n",
    "    Normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    image = Normalize(image).numpy()\n",
    "\n",
    "    if padding:  # padding\n",
    "        pad_to = max(h_new, w_new)\n",
    "        image, mask = pad_bottom_right(image, pad_to, ret_mask=True)\n",
    "        mask = torch.from_numpy(mask[0])\n",
    "    else:\n",
    "        mask = None\n",
    "\n",
    "    image = torch.from_numpy(image).float()[None]\n",
    "\n",
    "    return image, scale, mask, torch.tensor([h_new, w_new])\n",
    "\n",
    "class JamMa_Abstract(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35302228",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "        'coarse': {\n",
    "            'd_model': 256,\n",
    "        },\n",
    "        'fine': {\n",
    "            'd_model': 64,\n",
    "            'dsmax_temperature': 0.1,\n",
    "            'thr': 0.1,\n",
    "            'inference': True\n",
    "        },\n",
    "        'match_coarse': {\n",
    "            'thr': 0.2,\n",
    "            'use_sm': True,\n",
    "            'border_rm': 2,\n",
    "            'dsmax_temperature': 0.1,\n",
    "            'inference': True\n",
    "        },\n",
    "        'fine_window_size': 5,\n",
    "        'resolution': [8, 2]\n",
    "    }\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    #make sure to include valid paths to your images\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Image pair matching with JamMa',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        '--image1', type=str, default='../assets/figs/345822933_b5fb7b6feb_o.jpg',\n",
    "        help='Path to the source image')\n",
    "    parser.add_argument(\n",
    "        '--image2', type=str, default='../assets/figs/479605349_8aa68e066d_o.jpg',\n",
    "        help='Path to the target image')\n",
    "    parser.add_argument(\n",
    "        '--output_dir', type=str, default='output/',\n",
    "        help='Path of the outputs')\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "    \n",
    "    jamma = JamMa_Abstract(config=cfg).eval().to(device)\n",
    "\n",
    "    image0, scale0, mask0, prepad_size0 = read_megadepth_color(opt.image1, 832, 16, True)\n",
    "    image1, scale1, mask1, prepad_size1 = read_megadepth_color(opt.image2, 832, 16, True)\n",
    "\n",
    "    mask0 = F.interpolate(mask0[None, None].float(), scale_factor=0.125, mode='nearest', recompute_scale_factor=False)[0].bool()\n",
    "    mask1 = F.interpolate(mask1[None, None].float(), scale_factor=0.125, mode='nearest', recompute_scale_factor=False)[0].bool()\n",
    "    data = {\n",
    "        'imagec_0': image0.to(device),\n",
    "        'imagec_1': image1.to(device),\n",
    "        'mask0': mask0.to(device),\n",
    "        'mask1': mask1.to(device),\n",
    "    }\n",
    "\n",
    "    jamma(data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
